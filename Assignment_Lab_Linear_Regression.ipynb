{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_Lab_Linear Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPFboMrLUNUcOo/hFgjfwz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nutapol97/Python-for-DS-AI_Nutapol_T./blob/main/Assignment_Lab_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAr7Y-AQniXx",
        "outputId": "854eee17-b604-4e0e-9657-838fd2e7672e"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from time import time\n",
        "import math,random\n",
        "#assert X_train.shape[0] == y_train.shape[0]\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "m=X.shape[0]    # No. samples\n",
        "n=X.shape[1]    # No. features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "intercept = np.ones((X_train.shape[0], 1))\n",
        "X_train = np.concatenate((intercept, X_train), axis=1)\n",
        "intercept = np.ones((X_test.shape[0], 1))\n",
        "X_test = np.concatenate((intercept, X_test), axis=1)\n",
        "\n",
        "\n",
        "class Linear_Regression:\n",
        "\n",
        "  def __init__(self, alpha = 0.0001,max_iter = 10000000000,loss_old = 10000,\n",
        "               tol = 0.0001,method='Batch'):\n",
        "    self.alpha=alpha\n",
        "    self.max_iter=max_iter\n",
        "    self.loss_old= loss_old\n",
        "    self.tol=tol\n",
        "    self.method=method\n",
        "     \n",
        "    \n",
        "  def h_theta(self, X):\n",
        "        return X @ self.theta\n",
        "\n",
        "  def mse(self,yhat, y):\n",
        "      return ((yhat - y)**2).sum() / yhat.shape[0]\n",
        "\n",
        "  def gradient(self,X, error):\n",
        "      return X.T @ error\n",
        "  def sto(self,X_train,y_train):\n",
        "      no_random=random.randrange(X_train.shape[0])\n",
        "      X_train=X_train[no_random,:]\n",
        "      X_train=X_train.reshape(1,14)\n",
        "      y_train=y_train[no_random:no_random+1]\n",
        "      return X_train,y_train\n",
        "  def mini_batch(self,X_train,y_train,sizing):\n",
        "      random_np= np.random.randint(X_train.shape[0],size=sizing)\n",
        "      X_train=X_train[random_np,:]\n",
        "      y_train=y_train[random_np]\n",
        "      return X_train,y_train\n",
        "  def fit(self,X_train,y_train):\n",
        "      self.theta = np.zeros(X_train.shape[1]) \n",
        "      iter_stop=0\n",
        "      \n",
        "      theta = np.zeros(X_train.shape[1])\n",
        "      if self.method == 'sto':\n",
        "        X_train,y_train=self.sto(X_train,y_train)\n",
        "      if self.method == 'mini_batch':\n",
        "        X_train,y_train=self.mini_batch(X_train,y_train,300)\n",
        "      print(np.shape(X_train))\n",
        "      for i in range(self.max_iter):\n",
        "        yhat = self.h_theta(X_train)\n",
        "        loss_current = self.mse(yhat,y_train)       \n",
        "        different=np.abs(self.loss_old-loss_current)        \n",
        "        if different == self.tol or different < self.tol :\n",
        "            break\n",
        "        self.loss_old=loss_current\n",
        "        error = yhat - y_train\n",
        "        grad = self.gradient(X_train, error)\n",
        "        self.theta = self.theta - self.alpha * grad\n",
        "        iter_stop +=1\n",
        "       \n",
        "      print(\"iter_stop: \", iter_stop)\n",
        "        \n",
        "        \n",
        "model = Linear_Regression(method='sto')\n",
        "model.fit(X_train, y_train)\n",
        "yhat = model.h_theta(X_test)\n",
        "mse = model.mse(yhat, y_test)\n",
        "print(\"MSE: \", mse)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 14)\n",
            "iter_stop:  5326\n",
            "MSE:  530.9884344478036\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}