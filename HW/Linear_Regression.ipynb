{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7dFp0EF8EN/OO7epriiHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nutapol97/Python-for-DS-AI_Nutapol_T./blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqZVGQqd55g9"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5lcG2HCDFgF"
      },
      "source": [
        "Bunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hryipgM5E2l"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "boston = load_boston()"
      ],
      "execution_count": 632,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss9b8eO86i84"
      },
      "source": [
        "X = boston.data\n",
        "y = boston.target"
      ],
      "execution_count": 633,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRw2pXJg636Y"
      },
      "source": [
        "m=X.shape[0]    # No. samples\n",
        "n=X.shape[1]    # No. features"
      ],
      "execution_count": 634,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6FtZCOcQpLC"
      },
      "source": [
        "**Standardization of datasets** is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iuDyZ8G65_X"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 635,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNPrdRdzS0Sw"
      },
      "source": [
        "Split Datas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5oTzidQQ0kl"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ],
      "execution_count": 636,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLiLGT5OTLB_"
      },
      "source": [
        "Add intercepts\n",
        "\n",
        "np.concatenate : Join a sequence of arrays along an existing axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n8eVN-9TKZP",
        "outputId": "99ee3d04-b367-4cdc-ae11-9856559eb3ab"
      },
      "source": [
        "intercept = np.ones((X_train.shape[0], 1))\n",
        "X_train = np.concatenate((intercept, X_train), axis=1)\n",
        "np.shape(X_train)"
      ],
      "execution_count": 637,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(354, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 637
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S62VLDF8dhrC",
        "outputId": "5e3c0d95-14df-4baa-9af0-35b5ed91207e"
      },
      "source": [
        "intercept = np.ones((X_test.shape[0], 1))\n",
        "X_test = np.concatenate((intercept, X_test), axis=1)\n",
        "np.shape(X_test)"
      ],
      "execution_count": 638,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(152, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 638
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0qUnXavi7HO"
      },
      "source": [
        "Step 2: Fit your algorithm\n",
        "1. Define your algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYV0drrWenJk"
      },
      "source": [
        "from numpy.linalg import inv\n",
        "def closed_form(X, y):\n",
        "    return inv(X.T @ X) @ X.T @ y"
      ],
      "execution_count": 641,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eODpyZcjLhZ",
        "outputId": "778133f1-1075-487e-e275-e16c6bcae069"
      },
      "source": [
        "theta = closed_form(X_train, y_train)\n",
        "theta"
      ],
      "execution_count": 642,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.5114854 , -1.02908523,  1.14529114,  0.7216558 ,  0.29838682,\n",
              "       -2.06442942,  3.09418547, -0.0725698 , -3.14573302,  2.79193278,\n",
              "       -2.6574623 , -1.97708707,  0.94441182, -3.72884154])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 642
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp9EDsixjo3m",
        "outputId": "62375a81-a4d8-4345-bf5f-fcd44163c0a0"
      },
      "source": [
        "yhat = X_test @ theta\n",
        "yhat"
      ],
      "execution_count": 643,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.91225154, 23.71535879,  8.29744937, 24.65444106, 11.49053038,\n",
              "        9.15722108, 28.58457474, 21.86559229, 27.29621119, 39.51561257,\n",
              "       15.5023435 , 30.95271165, 22.98581825, 19.5120485 , 28.79894629,\n",
              "       18.27401951, 25.23033572, 28.58586844, 23.03391477, 20.32206395,\n",
              "       22.52863201, 30.04217991, 24.18476245, 10.67323183, 12.69197056,\n",
              "       21.28182104,  8.22891551, 19.38528571, 27.59639137, 24.55706324,\n",
              "       27.29300011, 15.91468939, 21.87681734, 13.54489741, 19.93272198,\n",
              "       28.7963338 , 18.08719504, 22.54932974, 33.50372441, 18.25590237,\n",
              "       13.93927185, 25.15862599, 14.05408303, 22.37432525, 18.19650791,\n",
              "       29.13014493, 24.71234489, 30.08913994, 21.34382517, 30.68083575,\n",
              "       18.00019991, 24.27254542, 24.1218011 , 29.05571932, 22.29187806,\n",
              "       29.19003065, 13.95210308, 25.83761815, 19.57916217, 28.65299977,\n",
              "       36.23738268,  4.95150792, 16.37936799, 21.55139064, 13.99717269,\n",
              "       21.21148236, 18.31743552, 19.07497848, 34.45984538, 30.0091869 ,\n",
              "       27.92270183, 29.37182691, 26.63649795, 25.94335981, 28.97861862,\n",
              "       21.26238706, 19.28441122, 17.30547343, 38.5617792 , 26.21417787,\n",
              "       12.63291569, 13.12930231, 20.35186049, 33.25288573, 18.8837584 ,\n",
              "       23.72614072, 36.87389673, 19.69106596,  9.09296952, 33.90247212,\n",
              "       25.30971881, 19.84671772, 21.22221674, 20.68909368, 25.40611444,\n",
              "       12.92069134, 25.31264277, 23.99458036, 18.65892706,  3.32121512,\n",
              "       28.74886968, 14.44494752, 25.91836512, 20.1366261 , 23.06800673,\n",
              "       17.84594471, 22.80665735, 22.12030715, 16.96163844, 30.06375986,\n",
              "       15.99045101, 17.17982673,  0.22658132, 17.18767985, 28.0299669 ,\n",
              "       21.50748731, 20.86365358, 20.32358023, 34.21726263, 43.13337323,\n",
              "       16.83024472, 17.65065762, 25.15227024, 20.22059314, 17.87671625,\n",
              "       21.28490492, 27.66175647, 33.32224594, -6.09586129, 19.8017317 ,\n",
              "       22.2781696 , 34.01275281, 32.96781975, 16.91397014, 31.09867555,\n",
              "       24.43610464, 19.30838425, 22.96889758, 24.42274675, 23.5357928 ,\n",
              "        7.11765453, 20.37099972, 15.44711999, 18.27708307,  5.27882837,\n",
              "       31.42211815, 21.38956073, 23.12282356, 27.01475833, 12.26156389,\n",
              "       15.79267158, 20.65736168])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 643
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYT6YW2rj1Pz"
      },
      "source": [
        "mse = ((y_test - yhat)**2).sum() / X_test.shape[0]"
      ],
      "execution_count": 645,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqQ6mmPMlSBo"
      },
      "source": [
        "Algorithm 2: Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyr9ciLFlRoe"
      },
      "source": [
        "from time import time\n",
        "\n",
        "# Step 1: Prepare your data\n",
        "# X_train, X_test have intercepts that are being concatenated to the data\n",
        "# [1, features\n",
        "#  1, features....]\n",
        "\n",
        "# making sure our X_train has same sample size as y_train\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "\n",
        "# initialize our w\n",
        "# We don't have to do X.shape[1] + 1 because our X_train already has the\n",
        "# intercept\n",
        "# w = theta/beta/coefficients\n",
        "theta = np.zeros(X_train.shape[1])\n",
        "\n",
        "# define the learning rate\n",
        "# later on, you gonna know that it should be better to make it slowly decreasing\n",
        "# once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
        "alpha = 0.0001\n",
        "\n",
        "# define our max_iter\n",
        "# typical to call it epochs <---ml people likes to call it\n",
        "max_iter = 1000\n",
        "\n",
        "loss_old = 10000\n",
        "\n",
        "tol = 0.0001\n",
        "\n",
        "iter_stop = 0\n",
        "\n",
        "def h_theta(X, theta): # theta dot X\n",
        "    return X @ theta\n",
        "\n",
        "def mse(yhat, y):    # The loss function is the mean squared error\n",
        "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
        "\n",
        "def gradient(X, error):\n",
        "    return X.T @ error\n",
        "\n",
        "start = time()\n",
        "\n",
        "# define your for loop\n",
        "for i in range(max_iter):\n",
        "    \n",
        "    # 1. yhat = X @ w\n",
        "    # prediction\n",
        "    # yhat (m, ) = (m, n) @ (n, )\n",
        "    yhat = h_theta(X_train, theta)\n",
        "\n",
        "    # 2. error = yhat - y_train\n",
        "    # error for use to calculate gradients\n",
        "    # error (m, ) = (m, ) - (m, )\n",
        "    error = yhat - y_train\n",
        "\n",
        "    # 3. grad = X.T @ error\n",
        "    # grad (n, ) = (n, m) @ (m, )\n",
        "    # grad for each feature j\n",
        "    grad = gradient(X_train, error)\n",
        "\n",
        "    # 4. w = w - alpha * grad\n",
        "    # update w\n",
        "    # w (n, ) = (n, ) - scalar * (n, )\n",
        "    theta = theta - alpha * grad  # update\n",
        "\n",
        "time_taken = time() - start"
      ],
      "execution_count": 715,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tidGJK7Rlmb-"
      },
      "source": [
        "2. Compute accuracy/loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUz_0ZExlmvI",
        "outputId": "fd807373-74b3-461d-ad52-16a355196d09"
      },
      "source": [
        "# we got our lovely w\n",
        "# now it's time to check our accuracy\n",
        "# 1. Make prediction\n",
        "yhat = h_theta(X_test, theta)\n",
        "\n",
        "# 2. Calculate mean squared errors\n",
        "mse = mse(yhat, y_test)\n",
        "\n",
        "# print the mse\n",
        "print(\"MSE: \", mse)\n",
        "print(\"Stop at iteration: \", iter_stop)\n",
        "print(\"Time used: \", time_taken)"
      ],
      "execution_count": 716,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  23.136055274319066\n",
            "Stop at iteration:  0\n",
            "Time used:  0.009975194931030273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqAnnnxUGqm2"
      },
      "source": [
        "\n",
        "=== Task ===\n",
        "\n",
        "1.Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPM88NSpGo5G",
        "outputId": "3e101b4b-3eee-4397-f765-4ce4efc0bd92"
      },
      "source": [
        "from time import time\n",
        "import math\n",
        "# making sure our X_train has same sample size as y_train\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "# Set theta at start point\n",
        "theta = np.zeros(X_train.shape[1])\n",
        "# Set Learning rate (alpha)\n",
        "alpha = 0.0001\n",
        "# Set Number of interlation that calculatetion\n",
        "max_iter = 1000\n",
        "# Set loss at start point for calculate. That should more big.\n",
        "loss_old = 10000\n",
        "# Ste tol for get the optmum point protect over fit\n",
        "tol = 0.0001\n",
        "iter_stop = 0\n",
        "\n",
        "def h_theta(X, theta):\n",
        "    return X @ theta\n",
        "\n",
        "def mse(yhat, y):\n",
        "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
        "\n",
        "def gradient(X, error):\n",
        "    return X.T @ error\n",
        "start = time()\n",
        "for i in range(max_iter):\n",
        "    \n",
        "    # 1. yhat = X @ w\n",
        "    # prediction\n",
        "    # yhat (m, ) = (m, n) @ (n, )\n",
        "    #### X train dot theta(Learning rate)####\n",
        "    yhat = h_theta(X_train, theta)\n",
        "    loss_current = mse(yhat,y_train)\n",
        "    \n",
        "    different=np.abs(loss_old-loss_current)\n",
        "    \n",
        "    if different == tol or different < tol:\n",
        "      break\n",
        "    loss_old=loss_current\n",
        "    # 2. error = yhat - y_train\n",
        "    # error for use to calculate gradients\n",
        "    # error (m, ) = (m, ) - (m, )\n",
        "    error = yhat - y_train\n",
        "    #print(error)\n",
        "    # 3. grad = X.T @ error\n",
        "    # grad (n, ) = (n, m) @ (m, )\n",
        "    # grad for each feature j\n",
        "    grad = gradient(X_train, error)\n",
        "\n",
        "    # 4. w = w - alpha * grad\n",
        "    # update w\n",
        "    # w (n, ) = (n, ) - scalar * (n, )\n",
        "    theta = theta - alpha * grad\n",
        "    iter_stop +=1\n",
        "time_taken = time() - start\n",
        "# we got our lovely w\n",
        "# now it's time to check our accuracy\n",
        "# 1. Make prediction\n",
        "yhat = h_theta(X_test, theta)\n",
        "\n",
        "# 2. Calculate mean squared errors\n",
        "mse = mse(yhat, y_test)\n",
        "\n",
        "# print the mse\n",
        "print(\"MSE: \", mse)\n",
        "print(\"Stop at iteration: \", iter_stop)\n",
        "print(\"Time used: \", time_taken)\n"
      ],
      "execution_count": 717,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  23.110826901477754\n",
            "Stop at iteration:  851\n",
            "Time used:  0.01662588119506836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjNmhYDfHmOY"
      },
      "source": [
        "\n",
        "\n",
        "1.    ***`assert`*** statement exists in almost every programming language. It helps detect problems early in your program, where the cause is clear, rather than later when some other operation fails.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "338QaZvCkedG"
      },
      "source": [
        "2.Implement options for stochastic gradient descent in which we use only one sample for training. Make sure that sample does not repeat unless all samples are read at least once already.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpedVQX6nyKI"
      },
      "source": [
        "Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YH9VZFDkbpc",
        "outputId": "6d030340-9f28-4d82-829f-8915b99394fd"
      },
      "source": [
        "from time import time\n",
        "import math,random\n",
        "# making sure our X_train has same sample size as y_train\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "\n",
        "# Set theta at start point\n",
        "theta = np.zeros(X_train.shape[1])\n",
        "# Set Learning rate (alpha)\n",
        "alpha = 0.0001\n",
        "# Set Number of interlation that calculatetion\n",
        "max_iter = 1000\n",
        "# Set loss at start point for calculate. That should more big.\n",
        "loss_old = 10000\n",
        "# Ste tol for get the optmum point protect over fit\n",
        "tol = 0.0001\n",
        "iter_stop = 0\n",
        "\n",
        "def h_theta(X, theta):\n",
        "    return X @ theta\n",
        "\n",
        "def mse(yhat, y):\n",
        "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
        "\n",
        "def gradient(X, error):\n",
        "    return X.T @ error\n",
        "\n",
        "def sto(X_train,y_train):\n",
        "  no_random=random.randrange(X_train.shape[0])\n",
        "  X_train=X_train[no_random,:]\n",
        "  X_train=X_train.reshape(1,14)\n",
        "  y_train=y_train[no_random:no_random+1]\n",
        " return X_train,y_train\n",
        "\n",
        "#X_train,y_train=sto(X_train,y_train)\n",
        "start = time()\n",
        "for i in range(max_iter):\n",
        "    \n",
        "    # 1. yhat = X @ w\n",
        "    # prediction\n",
        "    # yhat (m, ) = (m, n) @ (n, )\n",
        "    #### X train dot theta(Learning rate)####\n",
        "    yhat = h_theta(X_train, theta)\n",
        "    loss_current = mse(yhat,y_train)\n",
        "    \n",
        "    different=np.abs(loss_old-loss_current)\n",
        "    \n",
        "    if different == tol or different < tol:\n",
        "      break\n",
        "    loss_old=loss_current\n",
        "    # 2. error = yhat - y_train\n",
        "    # error for use to calculate gradients\n",
        "    # error (m, ) = (m, ) - (m, )\n",
        "    error = yhat - y_train\n",
        "    #print(error)\n",
        "    # 3. grad = X.T @ error\n",
        "    # grad (n, ) = (n, m) @ (m, )\n",
        "    # grad for each feature j\n",
        "    grad = gradient(X_train, error)\n",
        "\n",
        "    # 4. w = w - alpha * grad\n",
        "    # update w\n",
        "    # w (n, ) = (n, ) - scalar * (n, )\n",
        "    theta = theta - alpha * grad\n",
        "    iter_stop +=1\n",
        "time_taken = time() - start\n",
        "# we got our lovely w\n",
        "# now it's time to check our accuracy\n",
        "# 1. Make prediction\n",
        "yhat = h_theta(X_test, theta)\n",
        "\n",
        "# 2. Calculate mean squared errors\n",
        "mse = mse(yhat, y_test)\n",
        "\n",
        "# print the mse\n",
        "print(\"MSE: \", mse)\n",
        "print(\"Stop at iteration: \", iter_stop)\n",
        "print(\"Time used: \", time_taken)\n"
      ],
      "execution_count": 596,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  30.180133878751096\n",
            "Stop at iteration:  791\n",
            "Time used:  0.014987468719482422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFHwMB9GB7fm"
      },
      "source": [
        "3 .Add options for mini-batch gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXWOcwqdB7eO"
      },
      "source": [
        "Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ZfLMdh1n7N",
        "outputId": "cc0c3a5f-ef76-41be-f1e3-38642b8d661a"
      },
      "source": [
        "from time import time\n",
        "import math,random\n",
        "# making sure our X_train has same sample size as y_train\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "\n",
        "# Set theta at start point\n",
        "theta = np.zeros(X_train.shape[1])\n",
        "# Set Learning rate (alpha)\n",
        "alpha = 0.0001\n",
        "# Set Number of interlation that calculatetion\n",
        "max_iter = 1000\n",
        "# Set loss at start point for calculate. That should more big.\n",
        "loss_old = 10000\n",
        "# Ste tol for get the optmum point protect over fit\n",
        "tol = 0.0001\n",
        "iter_stop = 0\n",
        "\n",
        "def h_theta(X, theta):\n",
        "    return X @ theta\n",
        "\n",
        "def mse(yhat, y):\n",
        "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
        "\n",
        "def gradient(X, error):\n",
        "    return X.T @ error\n",
        "\n",
        "def mini_batch(X_train,y_train,sizing):\n",
        "  random_np= np.random.randint(X_train.shape[0],size=sizing)\n",
        "  X_train=X_train[random_np,:]\n",
        "  y_train=y_train[random_np]\n",
        "\n",
        "  return X_train,y_train\n",
        "\n",
        "X_train,y_train=mini_batch(X_train,y_train,300)\n",
        "start = time()\n",
        "for i in range(max_iter):\n",
        "    \n",
        "    # 1. yhat = X @ w\n",
        "    # prediction\n",
        "    # yhat (m, ) = (m, n) @ (n, )\n",
        "    #### X train dot theta(Learning rate)####\n",
        "    yhat = h_theta(X_train, theta)\n",
        "    loss_current = mse(yhat,y_train)\n",
        "    \n",
        "    different=np.abs(loss_old-loss_current)\n",
        "    \n",
        "    if different == tol or different < tol:\n",
        "      break\n",
        "    loss_old=loss_current\n",
        "    # 2. error = yhat - y_train\n",
        "    # error for use to calculate gradients\n",
        "    # error (m, ) = (m, ) - (m, )\n",
        "    error = yhat - y_train\n",
        "    #print(error)\n",
        "    # 3. grad = X.T @ error\n",
        "    # grad (n, ) = (n, m) @ (m, )\n",
        "    # grad for each feature j\n",
        "    grad = gradient(X_train, error)\n",
        "\n",
        "    # 4. w = w - alpha * grad\n",
        "    # update w\n",
        "    # w (n, ) = (n, ) - scalar * (n, )\n",
        "    theta = theta - alpha * grad\n",
        "    iter_stop +=1\n",
        "time_taken = time() - start\n",
        "# we got our lovely w\n",
        "# now it's time to check our accuracy\n",
        "# 1. Make prediction\n",
        "yhat = h_theta(X_test, theta)\n",
        "\n",
        "# 2. Calculate mean squared errors\n",
        "mse = mse(yhat, y_test)\n",
        "\n",
        "# print the mse\n",
        "print(\"MSE: \", mse)\n",
        "print(\"Stop at iteration: \", iter_stop)\n",
        "print(\"Time used: \", time_taken)\n"
      ],
      "execution_count": 606,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  37.7086932388522\n",
            "Stop at iteration:  1000\n",
            "Time used:  0.019178390502929688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gCNCXvQBzVL"
      },
      "source": [
        "Put everything into class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIXwJi-MBzCO",
        "outputId": "da8f9582-3706-433d-db10-fc09092d2833"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from time import time\n",
        "import math\n",
        "#assert X_train.shape[0] == y_train.shape[0]\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "m=X.shape[0]    # No. samples\n",
        "n=X.shape[1]    # No. features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "intercept = np.ones((X_train.shape[0], 1))\n",
        "X_train = np.concatenate((intercept, X_train), axis=1)\n",
        "intercept = np.ones((X_test.shape[0], 1))\n",
        "X_test = np.concatenate((intercept, X_test), axis=1)\n",
        "\n",
        "\n",
        "class Linear_Regression:\n",
        "\n",
        "  def __init__(self, alpha = 0.0001,max_iter = 1000,loss_old = 10000,\n",
        "               tol = 0.0001,method='Batch'):\n",
        "    self.alpha=alpha\n",
        "    self.max_iter=max_iter\n",
        "    self.loss_old= loss_old\n",
        "    self.tol=tol\n",
        "    self.method=method\n",
        "    \n",
        "  def h_theta(self,X,theta):\n",
        "      return X @ theta\n",
        "\n",
        "  def mse(self,yhat, y):\n",
        "      return ((yhat - y)**2).sum() / yhat.shape[0]\n",
        "\n",
        "  def gradient(self,X, error):\n",
        "      return X.T @ error\n",
        "  def sto(self,X_train,y_train):\n",
        "      no_random=random.randrange(X_train.shape[0])\n",
        "      X_train=X_train[no_random,:]\n",
        "      X_train=X_train.reshape(1,14)\n",
        "      y_train=y_train[no_random:no_random+1]\n",
        "      return X_train,y_train\n",
        "  def mini_batch(self,X_train,y_train,sizing):\n",
        "      random_np= np.random.randint(X_train.shape[0],size=sizing)\n",
        "      X_train=X_train[random_np,:]\n",
        "      y_train=y_train[random_np]\n",
        "      return X_train,y_train\n",
        "  def fit(self,X_train,y_train):\n",
        "      theta = np.zeros(X.shape[1])\n",
        "      iter_stop=0\n",
        "      loss_old=self.loss_old\n",
        "      theta = np.zeros(X_train.shape[1])\n",
        "      if self.method == 'sto':\n",
        "        X_train,y_train=self.sto(X_train,y_train)\n",
        "      if self.method == 'mini_batch':\n",
        "        X_train,y_train=self.mini_batch(X_train,y_train,300)\n",
        "      print(np.shape(X_train))\n",
        "      for i in range(self.max_iter):\n",
        "              \n",
        "        # 1. yhat = X @ w\n",
        "        # prediction\n",
        "        # yhat (m, ) = (m, n) @ (n, )\n",
        "        #### X train dot theta(Learning rate)####\n",
        "        yhat = self.h_theta(X_train,theta)\n",
        "        loss_current = self.mse(yhat,y_train)       \n",
        "        different=np.abs(loss_old-loss_current)        \n",
        "        if different == self.tol or different < self.tol:\n",
        "            break\n",
        "        loss_old=loss_current\n",
        "        # 2. error = yhat - y_train\n",
        "        # error for use to calculate gradients\n",
        "        # error (m, ) = (m, ) - (m, )\n",
        "        error = yhat - y_train\n",
        "        #print(error)\n",
        "        # 3. grad = X.T @ error\n",
        "        # grad (n, ) = (n, m) @ (m, )\n",
        "        # grad for each feature j\n",
        "        grad = self.gradient(X_train, error)\n",
        "\n",
        "        # 4. w = w - alpha * grad\n",
        "        # update w\n",
        "        # w (n, ) = (n, ) - scalar * (n, )\n",
        "        theta = theta - self.alpha * grad\n",
        "        iter_stop +=1\n",
        "          \n",
        "          \n",
        "model = Linear_Regression(method ='sto')\n",
        "model.fit(X_train, y_train)\n",
        "yhat = model.h_theta(X_test,theta)\n",
        "mse = model.mse(yhat, y_test)\n",
        "print(\"MSE: \", mse)\n",
        "print(\"iter_stop: \", iter_stop)"
      ],
      "execution_count": 731,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 14)\n",
            "MSE:  24.278351071417728\n",
            "iter_stop:  851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJFJ5LGIJrLj",
        "outputId": "8ffe8770-e126-489c-9365-777fd0521760"
      },
      "source": [
        "print(\"MSE: \", mse)"
      ],
      "execution_count": 732,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  24.278351071417728\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}