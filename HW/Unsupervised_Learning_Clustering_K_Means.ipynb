{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Learning - Clustering - K-Means.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhg7gFihG9t07KAnFhCF3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nutapol97/Python-for-DS-AI_Nutapol_T./blob/main/Unsupervised_Learning_Clustering_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRHuGTAVSxdq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import pairwise_distances_argmin\n",
        "from time import time\n",
        "from numpy.random import default_rng\n",
        "class KMeans:\n",
        "    def __init__(self, k, replacement=True, batch_size=60, max_iter=100):\n",
        "          self.k = k\n",
        "          self.replacement=replacement\n",
        "          self.batch_size = batch_size\n",
        "          self.max_iter = max_iter\n",
        "          \n",
        "    def mini_batch(self,X):\n",
        "      \n",
        "        rng = default_rng()     # function random not replace number\n",
        "        size_u=int(X.shape[0] * self.batch_size/100)\n",
        "        random_np=rng.choice(X.shape[0], size=size_u, replace=False)\n",
        "        X=X[random_np,:]\n",
        "        \n",
        "        return X\n",
        "    def kmeans(self,X, ):\n",
        "        m, n = X.shape\n",
        "        rng = np.random.RandomState(30)\n",
        "        i = rng.permutation(m)[:self.k]\n",
        "        self.centers = X[i]\n",
        "\n",
        "        #iteration = 0\n",
        "\n",
        "        for ix in np.arange(self.max_iter):\n",
        "            X_batch=self.mini_batch(X)\n",
        "\n",
        "\n",
        "            labels = pairwise_distances_argmin(X_batch, self.centers)\n",
        "\n",
        "            \n",
        "            new_centers = []\n",
        "\n",
        "            for i in range(self.k):\n",
        "                new_centers.append(X_batch[labels == i].mean(axis=0))\n",
        "            new_centers = np.array(new_centers)     \n",
        "            if(np.allclose(self.centers, new_centers)):\n",
        "                break\n",
        "            else:\n",
        "                self.centers = new_centers\n",
        "                 \n",
        "        print(f\"Done in {ix} iterations\")\n",
        "        total_variation_score = 0\n",
        "        labels = pairwise_distances_argmin(X, self.centers) \n",
        "        for i in range(self.k):\n",
        "            cluster_mean = X[labels==i].mean(axis=0)\n",
        "            total_variation_score += ((X[labels==i] - cluster_mean)** 2).sum()\n",
        "            \n",
        "        print(\"Total with variation score: \", total_variation_score)\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        return self.centers\n",
        "      \n",
        "    def predict(self,X):\n",
        "          return pairwise_distances_argmin(X, self.centers)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2o6LurlS1wO",
        "outputId": "fdc63a31-8d78-4e39-920e-e59964f2470b"
      },
      "source": [
        "X, y_true = make_blobs(n_samples=200, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "for k in range(1, 9):\n",
        "    print(f\"=====k = {k}\")\n",
        "    start = time()\n",
        "    model = KMeans(k)\n",
        "    model.kmeans(X)\n",
        "    preds = model.predict(X)\n",
        "    print(f\"Fit and predict time {time() - start}\")\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====k = 1\n",
            "Done in 99 iterations\n",
            "Total with variation score:  1829.1925956293885\n",
            "Fit and predict time 0.06265735626220703\n",
            "=====k = 2\n",
            "Done in 99 iterations\n",
            "Total with variation score:  913.4455966061479\n",
            "Fit and predict time 0.0566868782043457\n",
            "=====k = 3\n",
            "Done in 99 iterations\n",
            "Total with variation score:  345.8952943635978\n",
            "Fit and predict time 0.059192657470703125\n",
            "=====k = 4\n",
            "Done in 99 iterations\n",
            "Total with variation score:  334.9975317911792\n",
            "Fit and predict time 0.066864013671875\n",
            "=====k = 5\n",
            "Done in 99 iterations\n",
            "Total with variation score:  124.53984674817816\n",
            "Fit and predict time 0.06373262405395508\n",
            "=====k = 6\n",
            "Done in 99 iterations\n",
            "Total with variation score:  111.80593959909291\n",
            "Fit and predict time 0.06552410125732422\n",
            "=====k = 7\n",
            "Done in 99 iterations\n",
            "Total with variation score:  98.59181787868795\n",
            "Fit and predict time 0.08115553855895996\n",
            "=====k = 8\n",
            "Done in 99 iterations\n",
            "Total with variation score:  89.11069960824189\n",
            "Fit and predict time 0.06827092170715332\n"
          ]
        }
      ]
    }
  ]
}